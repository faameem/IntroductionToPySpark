* virtualenv is a tool to create isolated Python environments.
  https://virtualenv.pypa.io/en/stable/
  (1) Install virtualenv from above link
  (2) Create virtualenv directory --> $ virtualenv test-env
  (3) $ ls --> test-env
  (4) $ cd test-env --> bin include lib
  (5) $ . bin/activate
  (6) $ which python --> ~/test-env/bin/python

* Installing ipython in virtualenv
  (1) $ virtualenv ipython-env
  (2) $ cd ipython-env
  (3) $ . bin/activate
  (4) $ pip install ipython
  (5) $ ipython
  (6) []: ? // for help
  (7) []: %debug
  (8) ipdb> h // for help
  (9) ipdb> w // where am i
  (10) ipdb> p x // print x
  (11) ipdb> q // quit debugger
  (12) []: %pdb // automatic pdb calling has been turned on

* Installing Jupyter
  (1) pip install jupyter
  (2) $ jupyter notebook
  (3) Create new notebook and give it a title.
  (4) Work can be saved and shared, so good for reproducible research

* 2 spark shells (1) scala (2) python

* python spark shell
  (1) $ . pyspark/bin/activate // activate virtualenv
  (2) $ spark-*/bin/pypark // opens the pyspark repl in virtual env
  (3) >>> sc // spark context
  (4) >>> sqlContext // HiveContext
  
* Drivers (usually the shell) and Executors

* master configuration
$ pyspark --master=local[*] // * is the number of threads based on cores available

* Running IPYTHON in notebook
$ PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook" spark-*/bin/pyspark

* If we need an RDD to be used multiple times, then it makes sense to persist it using persist()

* When creating non-lambda function for map(), be it a pure function (self contained state)

* Pay attention to Python truthy/falsy concept in the filter.ipynb in chapter 4 In5-In13

* Python List[], Dict{}

* After filtering transformation, it makes sense to reduce the number of partitions by calling coalesce()

* Decrease the number of partitions using coalesce(), increase/decrease the number using Repartition()

* glom() returns the elements in a partition as a list/array.

* Fuction provided to reduce() should be both:
  associative = order of operation does not matter; addition/multiplication is associative, subtraction/division is not
  commutative = order of arguments does not matter; addition/multiplication is commutative, subtraction/division is not
  
* SaveAsTextFile:
  Common practise to repartition before saving to control the number of files generated per partition
  Having huge number of tiny files in HDFS will slow down performance
  Use Python saveAsPickleFile if we need to read back the data efficiently.
  Thrift can be used to read and write files too.

* ForEach:
  If the queue (collection) is available on each executor, then to see the result in the driver, we need to use the 
  accumulator or else each executor will discard the local queue after calculation.
  


