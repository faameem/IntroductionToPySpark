
* Drivers (usually the shell) and Executors

* master configuration
$ pyspark --master=local[*] // * is the number of threads based on cores available

* Running IPYTHON in notebook
$ PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook" pyspark

* If we need an RDD to be used multiple times, then it makes sense to persist it using persist()

* When creating non-lambda function for map(), be it a pure function (self contained state)

* Pay attention to Python truthy/falsy concept in the filter.ipynb in chapter 4 In5-In13

* Python List[], Dict{}

* After filtering transformation, it makes sense to reduce the number of partitions by calling coalesce()

* Decrease the number of partitions using coalesce(), increase/decrease the number using Repartition()

* glom() returns the elements in a partition as a list/array.

* Fuction provided to reduce() should be both:
  associative = order of operation does not matter; addition is associative, subtraction is not
  commutative = order of arguments does not matter; addition is commutative, subtraction is not
